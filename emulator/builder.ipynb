{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bit7b1f69f4b8694c11a491566f0cc14780",
   "language": "python",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as  sns\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_path = '../exp'\n",
    "exp_id = 'exp-1'\n",
    "\n",
    "if not os.path.exists(exp_id):\n",
    "    os.mkdir(exp_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Process log file to obtain external state data\n",
    "import os\n",
    "\n",
    "perf = pd.DataFrame([], columns=['instance', 'latency'])\n",
    "\n",
    "log_path = '%s/%s/logs' % (exp_path, exp_id)\n",
    "pub_logs = [x for x in os.listdir(log_path) if 'pub' in x]\n",
    "for p in pub_logs:\n",
    "    instance = p.split('.')[0]\n",
    "    with open('%s/%s' % (log_path, p)) as f:\n",
    "        lines = f.readlines()\n",
    "        latency = []\n",
    "        throughput = []\n",
    "        for l in lines:\n",
    "            if 'avg latency' in l:\n",
    "                try:\n",
    "                    lat = float(l.split(',')[2].replace(' ms avg latency', ''))\n",
    "                    thr = float(l.split(',')[1].split(' ')[1])\n",
    "                    # thr = float(l.split(',')[1].split('(')[1].split(' MB/sec')[0])\n",
    "                    latency.append(lat)\n",
    "                    throughput.append(thr)\n",
    "                except:\n",
    "                    print(l)\n",
    "                    continue\n",
    "        try:\n",
    "            df = {'instance': instance, 'latency': np.percentile(latency, 90), 'throughput': np.percentile(throughput, 90)}\n",
    "        except:\n",
    "            continue\n",
    "        perf = perf.append(df, ignore_index=True)\n",
    "\n",
    "perf.to_csv('tmp.csv')\n",
    "df = pd.read_csv('tmp.csv')\n",
    "ex_state = pd.DataFrame([], columns=['session', 'latency', 'throughput'])\n",
    "store = {}\n",
    "for index, row in df.iterrows():\n",
    "    sess = row['instance'].split('-')[2]\n",
    "    if sess not in store:\n",
    "        store.update({sess: {'latency': [row['latency']], 'throughput': [row['throughput']]}})\n",
    "    else:\n",
    "        store[sess]['latency'].append(row['latency'])\n",
    "        store[sess]['throughput'].append(row['throughput'])\n",
    "\n",
    "for sess in store:\n",
    "    ex_state = ex_state.append({\n",
    "        'session': sess, \n",
    "        'latency': np.mean(store[sess]['latency']), \n",
    "        'throughput': np.sum(store[sess]['throughput'])}, ignore_index=True)\n",
    "\n",
    "ex_state.to_csv('%s/%s/external-state.csv' % (exp_path, exp_id), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "\n",
    "X = pd.read_csv('%s/%s/schedule.csv' % (exp_path, exp_id))\n",
    "all_X = X.copy()\n",
    "\n",
    "Y = pd.read_csv('%s/%s/internal-state.csv' % (exp_path, exp_id))\n",
    "T = pd.read_csv('%s/%s/external-state.csv' % (exp_path, exp_id))\n",
    "\n",
    "features = pd.read_csv('../meta/state_meta.csv')['name'].to_list()\n",
    "meta_info = pd.read_csv('../meta/config_meta.csv')\n",
    "dt = meta_info['data_type']\n",
    "categorical_features = meta_info.loc[(meta_info['type'] == 'categorical')]\n",
    "for i, cf in categorical_features.iterrows():\n",
    "    options = cf['options'].split('/')\n",
    "    X[cf['name']] = X[cf['name']].apply(lambda x: options.index(x))\n",
    "\n",
    "# remove outliers from internal states\n",
    "lower = np.percentile(Y['dms_perf.server_broker_topics_AllTopicsBytesIn'], 1)\n",
    "upper = np.percentile(Y['dms_perf.server_broker_topics_AllTopicsBytesIn'], 99)\n",
    "\n",
    "# Select data between\n",
    "Y = Y[(Y['dms_perf.server_broker_topics_AllTopicsBytesIn'] > lower) & (Y['dms_perf.server_broker_topics_AllTopicsBytesIn'] < upper)]\n",
    "\n",
    "# remove outliers from internal states\n",
    "lower = np.percentile(T['latency'], 1)\n",
    "upper = np.percentile(T['latency'], 99)\n",
    "# Select data between\n",
    "T = T[(T['latency'] > lower) & (T['latency'] < upper)]\n",
    "\n",
    "Y['instance'] = Y['instance'].apply(lambda x: int(x.split('-')[2].split('_')[0]))\n",
    "\n",
    "instances = list(set(X['id']).intersection(T['session']).intersection(Y['instance']))\n",
    "\n",
    "X = X[X['id'].isin(instances)]\n",
    "Y = Y[Y['instance'].isin(instances)]\n",
    "T = T[T['session'].isin(instances)]\n",
    "\n",
    "missed_X = []\n",
    "for index, row in all_X.iterrows():\n",
    "    if row['id'] not in X['id']:\n",
    "        missed_X.append(row.to_dict())\n",
    "missed_X = pd.DataFrame(missed_X)\n",
    "missed_X.to_csv('%s/%s/missed.csv' % (exp_path, exp_id), index=None)\n",
    "\n",
    "X = X.sort_values(by=['id'])\n",
    "Y = Y.sort_values(by=['instance'])\n",
    "T = T.sort_values(by=['session'])\n",
    "\n",
    "X.set_index(\"id\", inplace=True)\n",
    "Y.set_index(\"instance\", inplace=True)\n",
    "T.set_index(\"session\", inplace=True)\n",
    "\n",
    "Y = Y[features]\n",
    "\n",
    "X_cols = np.array(X.columns)\n",
    "Y_cols = np.array(Y.columns)\n",
    "T_cols = np.array(T.columns)\n",
    "\n",
    "print(X.shape, Y.shape, T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data normalization\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
    "\n",
    "mms_X = MinMaxScaler().fit_transform(X)\n",
    "mms_Y = MinMaxScaler().fit_transform(Y)\n",
    "mms_T = MinMaxScaler().fit_transform(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature selection for throughput using Permutation\n",
    "import math\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "plt.rcParams.update({'figure.figsize': (10.0, 8.0)})\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "fig = plt.figure(figsize=(8, 16))\n",
    "# fig.tight_layout(pad=5)\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax2 = fig.add_subplot(212)\n",
    "ax = [ax1, ax2]\n",
    "\n",
    "for i, col in enumerate(T_cols):\n",
    "    rf = RandomForestRegressor(n_estimators=100)\n",
    "    rf.fit(mms_X, mms_T[:, i])\n",
    "\n",
    "    result = permutation_importance(rf, mms_X, mms_T[:, i], n_repeats=10, random_state=42, n_jobs=2)\n",
    "    importance = result.importances_mean\n",
    "    sorted_idx = importance.argsort()\n",
    "    ax[i].boxplot(result.importances[sorted_idx].T, vert=False, labels=X_cols[sorted_idx])\n",
    "    ax[i].set_xlabel(\"Random Forest Feature Importance\")\n",
    "    ax[i].set_title(col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Diemnsion reduction using PCA\n",
    "# from sklearn.decomposition import PCA \n",
    "# import seaborn as sns\n",
    "# from factor_analyzer.factor_analyzer import calculate_kmo, FactorAnalyzer\n",
    "\n",
    "# n_components = 3\n",
    "\n",
    "# # do Adequacy Test\n",
    "# kmo_all, kmo_model=calculate_kmo(mms_Y)\n",
    "# print('KMO:', kmo_model)\n",
    "# fa = FactorAnalyzer(n_components, rotation=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dr_data = fa.fit_transform(mms_Y)\n",
    "# ev1, v1 = fa.get_eigenvalues()\n",
    "# plt.title('Scree Plot')\n",
    "# plt.scatter(range(1, mms_Y.shape[1]+1), ev1)\n",
    "# plt.plot(range(1, mms_Y.shape[1]+1), ev1)\n",
    "# plt.plot(range(1, mms_Y.shape[1]+1), [1]*mms_Y.shape[1])\n",
    "# plt.xlabel('Factors')\n",
    "# plt.ylabel('Eigenvalue')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# var = fa.get_factor_variance() # 给出贡献率\n",
    "# print(\"\\nExplained Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cm = pd.DataFrame(np.abs(fa.loadings_), index=Y_cols)\n",
    "# ax = sns.heatmap(df_cm, annot=True, cmap=\"BuPu\")\n",
    "# ax.yaxis.set_tick_params(labelsize=15)\n",
    "# plt.title('Factor Analysis')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use Latent factors as output of prediction model\n",
    "# latent_Y_cols = np.array(['factor-%d' % i for i in range(n_components)])\n",
    "# latent_Y = MinMaxScaler().fit_transform(dr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Construct env prediction model using RF\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import mean_squared_error, median_absolute_error, mean_squared_log_error, mean_absolute_error, explained_variance_score, r2_score\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "def main(myY, myX, cols):\n",
    "    for i, metric in enumerate(cols):\n",
    "        print('--------- %s ---------' % metric)\n",
    "        y = myY[:, i].tolist()\n",
    "\n",
    "        # evaluate the model and collect the scores\n",
    "        best_model = None\n",
    "        best_score = -1e6\n",
    "        rf = RandomForestRegressor(random_state=1000)\n",
    "        gb = GradientBoostingRegressor(random_state=1000)\n",
    "        models = [rf, gb]\n",
    "        for model in models:\n",
    "            # define the evaluation procedure\n",
    "            cv = RepeatedKFold(n_splits=5, n_repeats=10, random_state=43)\n",
    "            n_scores = cross_val_score(model, myX, y, scoring='r2', cv=cv, n_jobs=-1)\n",
    "            if np.mean(n_scores) > best_score:\n",
    "                best_model = model\n",
    "                best_score = np.mean(n_scores)\n",
    "                model.fit(X=myX, y=y)\n",
    "                joblib.dump(best_model, '%s/%s.joblib' % (exp_id, metric))\n",
    "            print('r2: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))\n",
    "\n",
    "# main(latent_Y, mms_X, latent_Y_cols)\n",
    "main(mms_Y, mms_X, Y_cols)\n",
    "main(mms_T, mms_X, T_cols)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Validate prediction accurancy\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "cols = np.concatenate((Y_cols, T_cols), axis=0)\n",
    "myY = np.concatenate((mms_Y, mms_T), axis=1)\n",
    "\n",
    "w = 10\n",
    "h = 6\n",
    "\n",
    "fig, ax = plt.subplots(nrows=math.ceil(len(cols)/2), ncols=2, figsize=(w * 2, h * math.ceil(len(cols)/2)))\n",
    "fig.tight_layout(pad=6)\n",
    "\n",
    "titles = {\n",
    "    'dms_perf.blkio_io_service_bytes_recursive_total': 'container.blkio.io_service_bytes',\n",
    "    'dms_perf.cpu.usage_usermode': 'container.cpu.usage_usermode',\n",
    "    'dms_perf.cpu.usage_kernelmode': 'container.cpu.usage_kernelmode',\n",
    "    'dms_perf.memory.usage_total': 'container.memory.usage_total',\n",
    "    'dms_perf.os_process_cpu_time': 'kafka.os.process.cpu.time',\n",
    "    'dms_perf.server_broker_topics_TotalProduceRequestsPerSec': 'kafka.produce.request.per_sec',\n",
    "    'dms_perf.network_request_Produce_TotalTimeMs': 'kafka.produce.request.total_time',\n",
    "    'dms_perf.network_request_Produce_TemporaryMemoryBytes': 'kafka.produce.request.temporary_bytes',\n",
    "    'latency': 'latency',\n",
    "    'throughput': 'throughput'\n",
    "}\n",
    "\n",
    "for i, row in enumerate(ax):\n",
    "    for j, col in enumerate(row):\n",
    "        if i*len(row)+j >= len(cols):\n",
    "            continue\n",
    "        metric = cols[i*len(row)+j]\n",
    "        model = joblib.load('%s/%s.joblib' % (exp_id, metric))\n",
    "        predict_mmy = model.predict(mms_X)\n",
    "        k = list(cols).index(metric)\n",
    "        col.set_xlim(0, 1)\n",
    "        col.set_ylim(0, 1)\n",
    "        col.set_title(titles[metric], fontsize=20)\n",
    "        col.set_xlabel('Observed Value', fontsize=15)\n",
    "        col.set_ylabel('Predicted Value', fontsize=15)\n",
    "        col.grid()\n",
    "        col.scatter(myY[:, k], predict_mmy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "my_y_col = np.concatenate((Y_cols, T_cols), axis=0)\n",
    "my_y = np.concatenate((mms_Y, mms_T), axis=1)\n",
    "\n",
    "cols = np.concatenate((my_y_col, X_cols), axis=0)\n",
    "myY = np.concatenate((my_y, mms_X), axis=1)\n",
    "data = pd.DataFrame(myY, columns=cols)\n",
    "corr = data.corr('spearman')[X_cols].loc[my_y_col].abs()\n",
    "f, ax= plt.subplots(figsize = (14, 10))\n",
    "sns.heatmap(corr,cmap='YlGn', linewidths = 0.05, ax = ax, annot=True)\n",
    "ax.set_title('Correlation between features')\n",
    "f.savefig('corr.png', dpi=100, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data = pd.DataFrame(myY, columns=cols)\n",
    "corr = data.corr()\n",
    "# f, ax= plt.subplots(figsize = (14, 10))\n",
    "# sns.heatmap(corr,cmap='RdBu', linewidths = 0.05, ax = ax)\n",
    "# ax.set_title('Correlation between features')\n",
    "# f.savefig('corr.png', dpi=100, bbox_inches='tight')\n",
    "\n",
    "latency_corr = corr['latency']\n",
    "latency_corr = latency_corr[Y_cols]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "b = ax.barh(range(len(Y_cols)), latency_corr)\n",
    "\n",
    "# my_cols = [\n",
    "#     'container.blkio.io.service.bytes',\n",
    "#     'container.cpu.usage.total', \n",
    "#     'container.memory.usage.total',\n",
    "#     'container.cpu.usage.usermode', \n",
    "#     'container.cpu.usage.kernelmode',\n",
    "#     'jmx.network.request.produce.TemporaryMemoryBytes',\n",
    "#     'jmx.network.request.produce.TotalTimeMs',\n",
    "#     'jmx.server.broker.topics.TotalProduceRequestsPerSec',\n",
    "#     'jmx.gc.collection.time',\n",
    "#     'jmx.threading.thread.count',\n",
    "#     'jmx.threading.daemon.thread.count', \n",
    "#     'jmx.os.open.fd.count',\n",
    "#     'jmx.memory.non.heap.usage.used', \n",
    "#     'jmx.memory.heap.usage.used']\n",
    "my_cols = Y_cols\n",
    "ax.set_yticks(range(len(my_cols)))\n",
    "ax.set_yticklabels(my_cols)\n",
    "\n",
    "plt.title('latency correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data = pd.DataFrame(myY, columns=cols)\n",
    "corr = data.corr()\n",
    "\n",
    "thr_corr = corr['throughput']\n",
    "thr_corr = thr_corr[Y_cols]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "b = ax.barh(range(len(Y_cols)), thr_corr)\n",
    "\n",
    "ax.set_yticks(range(len(my_cols)))\n",
    "ax.set_yticklabels(my_cols)\n",
    "\n",
    "plt.title('throughput correlation')\n",
    "plt.show()"
   ]
  }
 ]
}